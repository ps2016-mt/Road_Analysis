{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scripts import load_data, analyse_missing_values, drop_unneeded_columns, plot_accidents_by_day_of_week, plot_accidents_by_month, plot_accidents_by_time, plot_distribution_share, preprocess_and_plot_correlation, check_unique_values, drop_missing_values, plot_feature_vs_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preview Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to read dataset\n",
    "df = load_data()\n",
    "\n",
    "# Load data head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically more than a million accidents, and currently 34 columns here. Let's have a deeper look at the data to investigate how we can proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-depth data overview to look at individual features, observe data type and accordingly process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check information about datatype for individual columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, quite a good mix of numerical and categorical features here, though a basic overview shows some features have quite a few missing variables. Let's investigate this in-depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and visualise this\n",
    "analyse_missing_values(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're seeing quite a few variables with a signficant share of missing variables, we should drop them as it is unlikely they will help in predictive modelling, and any imputes would likely be substantially sensitive to our assumptions, given that these are all objects. These are:\n",
    "1) Carriageway_Hazards\n",
    "2) Special_Conditions_at_Site\n",
    "3) 2nd_Road_Class\n",
    "4) 1st_Road_Class\n",
    "5) LSOA_of_Accident_Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Carriageway_Hazards', 'Special_Conditions_at_Site','2nd_Road_Class','1st_Road_Class','LSOA_of_Accident_Location']\n",
    "df = drop_unneeded_columns(df, columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also seeing quite a few time-related variables, including day of week, time (in hours), year etc. It would be cleaner if we create one DateTime column, so lets do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DateTime variable\n",
    "df['DateTime'] = df['Date']+' '+ df['Time']\n",
    "\n",
    "# Convert Datetime to pandas datetime\n",
    "\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'], format=\"%d/%m/%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the distribution of the accidents over the day of week, the hours in a day, and the months of the year, to see whether we can discern a trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accidents_by_day_of_week(df, datetime_col='DateTime')\n",
    "plot_accidents_by_month(df, datetime_col='DateTime')\n",
    "plot_accidents_by_time(df, datetime_col='DateTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accidents more likely to happen on a Fri, in October & November, and around rush hour (8am, and 4pm to 5pm). So there is a chance that Accident_Severity could be associated with the different time varaibles, simply due to higher frequency at certain points, so we should keep this in. That being said, we have quite a few time variables inside. What we'll do is drop all the time-variables, less DateTime, decompose DateTime into encoded variables for Month, Day of Week, and Hour of Day, for analysis, and then drop DateTime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop2 = ['Date', 'Day_of_Week','Time','Year']\n",
    "df = drop_unneeded_columns(df, columns_to_drop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Month\"] = df[\"DateTime\"].dt.month\n",
    "df[\"Day_of_Week\"] = df[\"DateTime\"].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "df[\"Hour_of_Day\"] = df[\"DateTime\"].dt.hour\n",
    "\n",
    "# Drop DateTime\n",
    "columns_to_drop3 = ['DateTime']\n",
    "df = drop_unneeded_columns(df, columns_to_drop3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have now our time variables as numeric features within our DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis & Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at our target; for this we will look at the values and the distribution within Accident_Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for readings within target variable\n",
    "unique_target_values = check_unique_values(df, column=\"Accident_Severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_share(df, 'Accident_Severity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large majority of accidents are classified as 'Slight', while roughly 15% of accidents are classidied as 'Serious'. Now, let us check the correlation of our target with the rest of the features, to get a better idea of which features will have better predictive power. We shall do this by creating a correlation matrix. To avoid altering the df that has been modified up till now, the function will create a copy to process, so that we can leave all encoding to the feature engineering aspect of this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target column and mapping\n",
    "target_column = \"Accident_Severity\"\n",
    "severity_mapping = {\"Slight\": 0, \"Serious\": 1, \"Fatal\": 2}\n",
    "\n",
    "# Select categorical columns (excluding the target column)\n",
    "categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "categorical_columns.remove(target_column)\n",
    "\n",
    "# Call the function\n",
    "preprocess_and_plot_correlation(df, target_column, categorical_columns, severity_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix will be useful in identifying the features we want to use for our modelling, adding some quantitative insights to some intution. For one, it is unlikely that granular location features will have that much of an effect on Accident_Severity, and this is backed by the correlation matrix. As such, we can drop the following indicators, for ease of analysis:\n",
    "\n",
    "#1. **`2nd_Road_Class`**: secondary road details are less relevant.\n",
    "\n",
    "#2. **`1st_Road_Class`**: redundant given other road-related features.\n",
    "\n",
    "#3. **`1st_Road_Number`**: road identifiers unlikely to predict severity.\n",
    "\n",
    "#4. **`Location_Easting_OSGR`, `Location_Northing_OSGR`, `Latitude`, `Longitude`**: Geographic coordinates too granular.\n",
    "\n",
    "#5. **`InScotland`**: Binary indicator not relevant given high level overview\n",
    "\n",
    "#6. **`Police_Force`**: Jurisdiction of police unlikely to affect severity of accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying columns\n",
    "columns_to_drop4 = ['1st_Road_Number','2nd_Road_Number','Latitude','Longitude',\n",
    "                      'Local_Authority_(Highway)','Local_Authority_(District)','Location_Easting_OSGR','Location_Northing_OSGR','InScotland', 'Police_Force']\n",
    "\n",
    "\n",
    "df_filtered = drop_unneeded_columns(df, columns_to_drop4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previewing data\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate the correlation matrix again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target column and mapping\n",
    "target_column = \"Accident_Severity\"\n",
    "severity_mapping = {\"Slight\": 0, \"Serious\": 1, \"Fatal\": 2}\n",
    "\n",
    "# Select categorical columns (excluding the target column)\n",
    "categorical_columns = df_filtered.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "categorical_columns.remove(target_column)\n",
    "\n",
    "# Call the function\n",
    "preprocess_and_plot_correlation(df_filtered, target_column, categorical_columns, severity_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's have a look at the data types within each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the unique values within each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_dict = check_unique_values(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the number of missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_missing_values(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, with a million data points, the largest number of missing's within a column amounts to 21,000. That's relatively small given the number of data points, so rather than going through the trouble of making a category for missing, we can drop them out of the equation entirely, without massively affecting predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = drop_missing_values(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_dict = check_unique_values(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better now, for the Junction_Control, Urban_or_Rural_Area, and Junction_Detail, there technically is a ready made category which denotes missing via the 'Data Missing' or 'Unallocated' fields. This should conclude the processing, and we are ready to now save this as a parquet file to proceed with the rest of the modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final processed dataset to a .parquet file\n",
    "df_filtered.to_parquet(\"data/processed_data.parquet\", index=False)\n",
    "print(\"Processed data saved to data/processed_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below section is additional analysis to look at the distribution of the different features against the target, just to provide some context into how different features could affect the target, along with some multicollinearity instances across different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the effect of weather conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather conditions, including rain, has a strong likelihood of increasing accident prevalence, and by extension, severity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the weather variable\n",
    "plot_distribution_share(df_filtered, \"Weather_Conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, most accidents happened with no adverse weather conditions, let's now look at how the weather conditions vary with our target, to see whether severity could have been affected by weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the feature against the target\n",
    "plot_feature_vs_target(df_filtered, \"Weather_Conditions\", \"Accident_Severity\", kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each severity, most accidents happened with no adverse weather conditions, though for all other weather conditons less fine and no high winds, this was more evenly distributed, suggesting that there is a chance that weather conditions could have made an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the effect of road conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps weather conditions is more of a secondary variable influencing road conditions (as evidenced by the relatively stronger correlation between the two variables in our correlation matrix). Wet roads due to heavy rain, or icy roads due to snow could have significant affect on a drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the road surface variable\n",
    "plot_distribution_share(df_filtered, \"Road_Surface_Conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most accidents happened on dry roads, but we're also seeing quite a few accidents on wet or damp roads, which aligns with our hypthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the road surface conditions against weather conditions\n",
    "plot_feature_vs_target(df_filtered, \"Road_Surface_Conditions\", \"Weather_Conditions\", kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our hypothesis seems right here, wet weather conditions lead to wet roads, which affect drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the road surface conditions against weather conditions\n",
    "plot_feature_vs_target(df_filtered, \"Road_Surface_Conditions\", \"Accident_Severity\", kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into rural-urban splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of each severity within Urban/Rural areas\n",
    "proportions = (\n",
    "    df_filtered.groupby(\"Urban_or_Rural_Area\")[\"Accident_Severity\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .rename(\"Proportion\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Plot the proportions\n",
    "sns.barplot(\n",
    "    x=\"Urban_or_Rural_Area\", \n",
    "    y=\"Proportion\", \n",
    "    hue=\"Accident_Severity\", \n",
    "    data=proportions\n",
    ")\n",
    "plt.title(\"Share of Accident Severity by Urban vs Rural Area\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.xlabel(\"Urban or Rural Area\")\n",
    "plt.legend(title=\"Accident Severity\", loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rural areas more likely to be associated with serious and fatal accidents, likely due to to higher speed limits into rural areas, which explains the high correlation between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Accident_Severity\", y=\"Speed_limit\", data=df_filtered)\n",
    "plt.title(\"Speed Limit vs Accident Severity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like accident severity rating increases with speed limit, at least from slight, to severe and fatal. Higher speed thresholds associated with impact, which would in turn feed into severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Accident_Severity\", y=\"Number_of_Casualties\", data=df_filtered)\n",
    "plt.title(\"Number of Casualties vs Accident Severity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casualities dosen't have an explicit link with severtiy, though 'Fatal' accidents sees greater outlier magnittude, and on average, Severe and Fatal accidents have wider boxes, indicating larger spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Accident_Severity and Police_Attendance to calculate proportions\n",
    "attendance_severity = df_filtered.groupby([\"Accident_Severity\", \"Did_Police_Officer_Attend_Scene_of_Accident\"]).size().unstack()\n",
    "\n",
    "# Normalize to calculate the percentage share of each attendance category\n",
    "attendance_share = attendance_severity.div(attendance_severity.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "attendance_share.plot(kind=\"bar\", stacked=True, figsize=(10, 6), colormap=\"viridis\")\n",
    "plt.title(\"Share of Police Attendance Categories by Accident Severity\")\n",
    "plt.xlabel(\"Accident Severity\")\n",
    "plt.ylabel(\"Share\")\n",
    "plt.legend(title=\"Police Attendance\", loc=\"upper right\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For context,\n",
    "\n",
    "1.0 = police attended;\n",
    "2.0 = police did not attend\n",
    "3.0 = processed via self-completion form\n",
    "\n",
    "It seems like police attendance was more likely for increasing scales of accident severity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "road_traffic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
