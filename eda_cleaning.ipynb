{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scripts import load_data, drop_unneeded_columns, plot_distribution, plot_feature_vs_target, handle_missing_values, plot_accidents_by_time, plot_accidents_by_month, plot_accidents_by_day_of_week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function to read dataset\n",
    "data = load_data()\n",
    "\n",
    "# load data head\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall data overview and contained features\n",
    "### Conduct wrangling, fix missing values, and overall cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check information about datatype for individual columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summaries for numerical columns\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum().sort_values(ascending=False)\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "print(pd.DataFrame({\"Missing Values\": missing_values, \"Percentage\": missing_percentage}))\n",
    "\n",
    "# Visualise missing data\n",
    "sns.heatmap(data.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're seeing quite a few variables with a signficant share of missing variables, we should drop them as it is unlikely they will help in predictive modelling, and any imputes would likely be substantially sensitive to our assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Carriageway_Hazards', 'Special_Conditions_at_Site','2nd_Road_Class','1st_Road_Class','LSOA_of_Accident_Location']\n",
    "drop_unneeded_columns(data, columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis & Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying columns to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying columns\n",
    "columns_to_analyse = ['Accident_Severity','Date','Day_of_Week','Did_Police_Officer_Attend_Scene_of_Accident','Latitude','Longitude',\n",
    "                      'Light_Conditions','Local_Authority_(District)','Number_of_Casualties','Road_Surface_Conditions',\n",
    "                      'Road_Type','Speed_limit','Time','Urban_or_Rural_Area', 'Weather_Conditions','Year']\n",
    "\n",
    "# Reasons to drop:\n",
    "#1. **`2nd_Road_Class`**: secondary road details are less relevant.\n",
    "#2. **`1st_Road_Class`**: redundant given other road-related features.\n",
    "#3. **`LSOA_of_Accident_Location`**: too granular for analysis.\n",
    "#4. **`2nd_Road_Number`**: road identifiers unlikely to predict severity.\n",
    "#5. **`Location_Easting_OSGR`, `Location_Northing_OSGR`**: Geographic coordinates too granular.\n",
    "#6. **`InScotland`**: Binary indicator; less relevant in its current form.\n",
    "\n",
    "data_filtered = data[columns_to_analyse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previewing data\n",
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values within filtered dataframe\n",
    "missing_values = data_filtered.isnull().sum().sort_values(ascending=False)\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "print(pd.DataFrame({\"Missing Values\": missing_values, \"Percentage\": missing_percentage}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call in a function to drop rows with missing values, given the top features with missing values now are mostly objects, wouldn't make a lot of sense imputing. Moreover, given we have quite a bit of data post drop, we should be ok to still proceed without significantly affecting prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = handle_missing_values(df=data_filtered, drop_na_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for readings within target variable\n",
    "target = data_filtered['Accident_Severity']\n",
    "target_values = target.unique()\n",
    "target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable\n",
    "plot_distribution(data_filtered, \"Accident_Severity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local authority analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at prevalence by local authority district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.rename(columns={'Local_Authority_(District)':'LA'},inplace=True)\n",
    "data_filtered.head()\n",
    "# top 30 authorities by accident\n",
    "LA_by_collisions = data_filtered.LA.value_counts(ascending=False)\n",
    "LA_by_collisions[:30]\n",
    "LA_by_collisions[:30].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birmingham showing the highest prevalence of accidents, followed by Leeds, and Manchester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the effect of weather conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather conditions, including rain, has a strong likelihood of increasing accident prevalence, and by extension, severity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for readings within weather variable\n",
    "target = data_filtered['Weather_Conditions']\n",
    "target_values = target.unique()\n",
    "target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the weather variable\n",
    "plot_distribution(data_filtered, \"Weather_Conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, most accidents happened with no adverse weather conditions, let's now look at how the weather conditions vary with our target, to see whether severity could have been affected by weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the feature against the target\n",
    "plot_feature_vs_target(data_filtered, \"Weather_Conditions\", \"Accident_Severity\", kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each severity, most accidents happened with no adverse weather conditions, though for all other weather conditons less fine and no high winds, this was more evenly distributed, suggesting that there is a chance that weather conditions could have made an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the effect of road conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps weather conditions is more of a secondary variable influencing road conditions. Wet roads due to heavy rain, or icy roads due to snow could have significant affect on a drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for readings within road surface variable\n",
    "target = data_filtered['Road_Surface_Conditions']\n",
    "target_values = target.unique()\n",
    "target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the road surface variable\n",
    "plot_distribution(data_filtered, \"Road_Surface_Conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most accidents happened on dry roads, but we're also seeing quite a few accidents on wet or damp roads, which aligns with our hypthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the road surface conditions against weather conditions\n",
    "plot_feature_vs_target(data_filtered, \"Road_Surface_Conditions\", \"Weather_Conditions\", kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our hypothesis seems right here, wet weather conditions lead to wet roads, which affect drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the road surface conditions against weather conditions\n",
    "plot_feature_vs_target(data_filtered, \"Road_Surface_Conditions\", \"Accident_Severity\", kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the idea of date and time, and whether certain dates or times are associated with greater severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date\n",
    "data_filtered['Date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time\n",
    "data_filtered['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DateTime variable\n",
    "data_filtered['DateTime'] = data_filtered['Date']+' '+ data_filtered['Time']\n",
    "# Convert Datetime to pandas datetime\n",
    "data_filtered['DateTime'] = pd.to_datetime(data_filtered['DateTime'], format=\"%d/%m/%Y %H:%M\")\n",
    "data_filtered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accidents_by_day_of_week(data_filtered)  # Plot by date\n",
    "plot_accidents_by_month(data_filtered)  # Plot by month\n",
    "plot_accidents_by_time(data_filtered)  # Plot by time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accidents more likely to happen on a Fri, in October & November, and around rush hour (8am, and 4pm to 5pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of features with target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the feature types within the current iteration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there is still quite a bit of pre-processing we must do here to get this ready for the modelling. First, lets pull out a correlation heatmap to guide our feature selection, that way we can do a bit of pre-processing automatically, and also check which features to choose, based on coorelation with our target (Accident Severity) and also pinpoint indicators with high multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "irrelevant_cols = [\"Date\", \"Time\", \"Month\", \"Year\", \"Hour\"]\n",
    "data_filtered = data_filtered.drop(columns=irrelevant_cols, errors=\"ignore\")\n",
    "\n",
    "# Encode target variable (Accident_Severity)\n",
    "severity_mapping = {\"Slight\": 0, \"Serious\": 1, \"Fatal\": 2}\n",
    "data_filtered[\"Accident_Severity\"] = data_filtered[\"Accident_Severity\"].map(severity_mapping)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = data_filtered.select_dtypes(include=[\"object\"]).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data_filtered[col] = le.fit_transform(data_filtered[col].astype(str))  # Convert to string for consistency\n",
    "    label_encoders[col] = le  # Save encoder for future use\n",
    "\n",
    "# Handle missing values (drop rows with missing values)\n",
    "data_filtered = data_filtered.dropna()\n",
    "\n",
    "# Draw correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = data_filtered.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap of Features\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "road_traffic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
